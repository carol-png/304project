---
title: "New York uber taxi price analysis"
author: "Zishan Cheng"
date: "16/12/2020"
fontsize: 12pt
output:
  pdf_document: default
  html_document:
    df_print: paged
bibliography: "references.bib"
biblio-style: alphadin
link-citations: true
abstract: "Uber is raised quickly in recent year due to its efficiency. Unlike the traditional taxi economy, online reservation is provided. In this project, we would like to analysis the average price of uber in different time of a day in New York city by ratio and regression estimation methods. Based on our analysis, we can tell that the average price per mile is approximate $7.03/mile; the greatest varibility of the price happens at rush hour, especially 9 am.\\par
 \\textbf{Keywords:} uber, stratified, ratio, regression \\par
 \\textbf{Github:} https://github.com/carol-png/304project"
---

\newpage

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(rasterly)
library(magrittr)
library(gridExtra)
library(grid)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Uber, is an American multinational ride-hailing company providing services that include peer-to-peer ride sharing, food delivery (Uber Eats), and etc. The company is based in San Francisco and has operations in over 785 metropolitan areas worldwide. Its platforms can be accessed via its websites and mobile apps [@uber]. Compared with the traditional taxi economy, reservation can be set online to make the , additionally, the price of uber is fluid via the demand. 

In this project, we would like to investigate unit uber prices (price per mile) in New York at different time of a day. This research could give a guidance that at which hour, uber drivers are more likely to earn above the average; at which hour, uber drivers are less likely to earn above the average. To uncover it, ratio and regression estimation under stratified sampling (each hour as ), with focuses on point estimation and confidence intervals is applied for this analysis. 

# Methodology

## Data

Our data "[uber](https://examples.pyviz.org/nyc_taxi/nyc_taxi-nongeo.html)" is collected via `dashshader` website [@rougier2013shader]. It involves all background data source in New York city (mainly Manhattan Island), on Jan 2015. The variables our data containing are 'pickup location x', 'pickup location y', 'total miles', 'total payment' and etc. The **target population** of our study is all activated uber taxi drivers in New York in the past 11 years (uber is founded in 2009) and further years. The **sample population** is activated uber taxi drivers in New York in Jan, 2015 and the sample size, $N = 10679307$ (however, such data set is around 750 MB which is almost impossible to upload on *Quercus*. To accommodate the size of file less than 25 MB, we randomly picked 1e6 observations). 

The target variable is 'price per mile', defined as
\[\text{price per mile} = \frac{\text{total payment}}{\text{total miles}}\]
Since all variables are numerical, we can remove observations containing any `NA`, `NaN`, and infinite values. Then, let us visualize the histograms of variable 'price per mile' and 'total miles'.

```{r, echo=FALSE, fig.cap="Price and Trip Histograms", warning=FALSE, error = FALSE, message=FALSE, fig.width=6, fig.height=3, fig.align="center"}
d <- read_csv("newd.csv")
d %<>% 
  mutate(pick_up_hour = as.numeric(pick_up_hour)) %>%
  filter_if(~is.numeric(.), 
            all_vars(!is.infinite(.))) %>%
  na.omit()
par(mfrow = c(1,2))
hist(d$amount_per_mile, main = "Histogram", 
     xlab = "price pre mile")
hist(d$trip_distance, main = "Histogram", 
     xlab = "trip distance")
d %<>%
  filter(amount_per_mile < 20 & amount_per_mile > 0 &
           trip_distance < 20 & trip_distance > 0)
```

Through this Figure, we would tell that there are extremely large values, even close to `6e5` and some negative values in these two variables. We suppose it is caused by incorrect recordings of uber system. To better investigate the relationship, such outliers are removed. In our case, we set the boundary of 'price per mile' and 'total miles' as  $(0, 20]$ and $(0, 30]$, respectively. 

The Figure 2 illustrates the overall 'price per mile' in Manhattan Island. Coordinate $x$ and $y$ represent latitude and longitude, respectively. The shade of color represents the 'price per mile'. If the color in one region is bright which means that the 'price per mile' at this pick up location is higher. Obviously, passages at the left bottom corner of Manhattan Island are less willing to pay tips than other regions.

```{r, echo=FALSE, fig.cap="Manhattan Heat Map (colour represents the unit price)", warning=FALSE, error = FALSE, message=FALSE, fig.width=3, fig.height=3, fig.align="center"}
d %>% 
  rasterly(aes(x = pickup_x,
               y = pickup_y,
               on = amount_per_mile),
           background = "black",
           color = fire_map,
           reduction_func = "mean"
  ) %>% 
  rasterly_points()
```

In our analysis, we will choose trip distance as our auxiliary variable. However, the left one of Figure 3 shows that the correlation between 'total miles' and 'price per mile' is negative and they are non-linear. In both ratio and regression stratified sampling, we assume the relationship is linear. Thus, we would like to perform power transformation [@box1964analysis] on these two variables. Based on the shape, we can decrease the power of $x$ and $y$, 
\[x^* = log(x)\]
\[y^* = log(y)\]
The right one of Figure 3 shows that the relationship of 'total miles' and 'price per mile' is almost linear.

```{r, echo=FALSE, fig.cap="Relationship between Trip Distance and Payment per Mile", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}
g1 <- d %>% 
  rasterly(aes(x = trip_distance,
               y = amount_per_mile)
  ) %>% 
  rasterly_points() %>%
  rasterlyGrob(xlab = "Trip Distance", 
               ylab = "Payment per mile", 
               legend_gpar = grid::gpar(col = "black", cex = 0.8),
               main = "Non-linear Relationship")
g2 <- d %>% 
  rasterly(aes(x = log(trip_distance),
               y = log(amount_per_mile))
  ) %>% 
  rasterly_points() %>% 
  rasterlyGrob(xlab = "log(Trip Distance)", 
               ylab = "log(Payment per mile)",
               legend_gpar = grid::gpar(col = "black", cex = 0.8),
               main = "Linear Relationship",
               ylim = c(-2, 3))
gridExtra::grid.arrange(grobs = list(g1, g2), nrow = 1)
```

## Model

```{r, echo=FALSE, fig.cap="Transformed Data", warning=FALSE, error = FALSE, message=FALSE, fig.width=4, fig.height=3, fig.align="center"}
N <- dim(d)[1]
```
  
Ratio estimation and regression estimation [@singh2013elements] are often performed when the target variable is hard to obtain but the auxiliary variable is relatively easier to obtain. Additionally, the relationship between target variable and auxiliary variable should be strong. 

In our case, the 'total miles' can be observed by uber drivers before the start of the trip, while, the 'price per mile' largely depends on traffic and the generosity of passages. Due to the strong relationship between 'price per mile' and 'total miles', it is suitable to use ratio estimation and regression estimation in our case.
  
```{r, echo=FALSE, fig.cap="Transformed Data", warning=FALSE, error = FALSE, message=FALSE, fig.width=4, fig.height=3, fig.align="center"}
Nh <- table(d$pick_up_hour)
Nh <- as.vector(Nh)
wh <- Nh/N
n <- 1e5
# Samples
set.seed(857)
# log transformation
dlog <- d %>% 
  mutate(log_trip_distance = log(trip_distance), 
         log_amount_per_mile = log(amount_per_mile))
H <- seq(24)
yh <- list()
xuh <- list()
yuh <- list()
xh <- list()
nh <- round(wh * n)
hide <- lapply(H-1,
               function(i) {
                 dlog %>%
                   filter(pick_up_hour == i) -> ds
                 xuh <<- c(xuh, list(ds$log_trip_distance))
                 yuh <<- c(yuh, list(ds$log_amount_per_mile))
                 ns <- dim(ds)[1]
                 nh <- nh[i+1]
                 s <- sample(1:ns, nh, replace = FALSE)
                 ds <- ds[s, ]
                 xh <<- c(xh, list(ds$log_trip_distance))
                 yh <<- c(yh, list(ds$log_amount_per_mile))
               })
```

### Ratio estimation under stratified sampling

* Separate Ratio Estimator Method:

  Ratio is defined as 
  \[\hat{R_{SR}} = \frac{\bar{{y}_h}}{\bar{{x}_h}}\]
  
  where $\bar{{y}_h}$ represents the average price per mile, $\bar{{x}_h}$ represents the average total miles. 

  The strata mean is
  \[\hat{{y}_{Uh}} = \hat{R_{SR}}{x}_{Uh}\]
  where ${x}_{Uh}$ represents average total miles at given hour $h$

  The separate ratio estimate of $\hat{{y}_{U}}$ is 
  \[\hat{{y}_{U}} = \sum^H_{h = 1} w_h\hat{{y}_{Uh}}\]
  where $w_h = \frac{N_h}{N}$ represents weights of such strata; $N_h$ is the sample population size of hour $h$ and $N$ is the sample population size; ${y}_{Uh}$ represents average price per mile at given hour $h$.

```{r, echo=FALSE}
RSeparateRatio <- vapply(seq(length(yh)),
                         function(i) {
                           mean(yh[[i]])/mean(xh[[i]])
                         }, numeric(1))
xu <- vapply(xuh, mean, numeric(1))
ySeparateRatioStrata <- RSeparateRatio * xu
ySeparateRatio <- sum(wh * ySeparateRatioStrata)
```

  The variance of $y_u$ is defined as
  \[V(y_u) = \sum^H_{h = 1}[w_h^2\frac{N_h - n_h}{N_hn_h(n_h-1)}\sum_{i = 1}^{n_h}(y_{hi} - \hat{R_{SR}}x_{hi})^2]\]
  where $n_h$ is the resample size at hour $h$.

```{r, echo=FALSE}
getVarsRatio <- function(Nh, nh, H, yh, R, xh) {
  if(length(R) == 1) R <- rep(R, length(H))
  (Nh - nh)/(Nh * nh * (nh - 1)) * 
      vapply(H, function(i) 
        sum((yh[[i]] - R[i] * xh[[i]])^2), numeric(1))
}
varsSeparateRatioStrata <- getVarsRatio(Nh, nh, H, yh, 
                                        RSeparateRatio, xh)
varSeparateRatio <- sum(wh^2 * varsSeparateRatioStrata )
```

* Combined Ratio Estimator Method:

  Based on weighted least square
  \[\hat{R_{CR}} = \frac{{y}_{st}}{{x}_{st}}\]
  where ${y}_{st} = \sum \frac{N_h}{N}\bar{y}_h$ and ${x}_{st} = \sum \frac{N_h}{N}\bar{x}_h$
  thus 
  \[\hat{\mu_{y}} = \hat{R_{CR}}\mu_{x}\]
  where $\mu_{x}$ is the estimate the population mean of `total miles`

```{r, echo=FALSE}
RCombinedRatio <- sum(wh * vapply(yh, mean, numeric(1)))/
  sum(wh * vapply(xh, mean, numeric(1)))
yCombinedRatio <- RCombinedRatio * mean(unlist(xh))
```

  The variance is
  \[V(y_u) = \sum^H_{h = 1}[w_h^2\frac{N_h - n_h}{N_hn_h(n_h-1)}\sum_{i = 1}^{n_h}(y_{hi} - \hat{R_{CR}}x_{hi})^2]\]

```{r, echo=FALSE}
varsCombinedRatioStrata <- (Nh - nh)/(Nh * nh * (nh - 1)) * 
  vapply(H, function(i) 
    sum((yh[[i]] - RCombinedRatio * 
           xh[[i]])^2), numeric(1))
varCombinedRatio <- sum(wh^2 * varsCombinedRatioStrata)
```

### Regression estimation under stratified sampling

* Separate Regression Estimator Method:

  Based on origin least square, $\hat{a}_h$ and $\hat{b}_h$ can be obtained, thus 

  \[\hat{{y}_{Uh}} = \hat{a}_h + \hat{b}_h {x}_{Uh}\]

  estimate of $\hat{{y}_{U}}$ is 

  \[\hat{{y}_{U}} = \sum^H_{h = 1} \frac{N_h}{N}\hat{{y}_{Uh}}\]

```{r, echo=FALSE}
ah <- c()
bh <- c()
separateRes <- lapply(H, 
                      function(h) {
                        fit <- lm(yh[[h]]~xh[[h]])
                        ah <<- c(ah, fit$coefficients[1])
                        bh <<- c(bh, fit$coefficients[2])
                        fit$residuals
                      })
ySeparateRegressionStrata <- unname(ah + bh * xu)
ySeparateRegression <- sum(wh * ySeparateRegressionStrata)
```

  The variance is
  \[Var = \sum^H_{h = 1}[w_h^2\frac{1 - w_h}{n_h(n_h-2)}SSE_h^2]\]
  where $SSE_h$ is the error sum of squares at hour $h$.

```{r, echo=FALSE}
getVarsRegression <- function(wh, nh, res) {
  (1 - wh) /(nh*(nh - 2)) * vapply(res, 
                                   function(r) sum(r^2), numeric(1))
}
varsSeparateRegressionStrata <- 
  getVarsRegression(wh, nh, separateRes)
varsSeparateRegression <- sum(wh^2 * 
                                varsSeparateRegressionStrata)
```

* Combined Regression Estimator Method:

  The $\hat{{y}_{U}}$ can be obtained by

  \[\hat{{y}_{U}} = \hat{{y}_{st}} + b_c(\hat{{x}_{U}} - \hat{{x}_{st}})\]

  where $b_c = \frac{\sum_{h = 1}^H c_hb_h}{\sum_{h = 1}^H c_h}$ and $c_h = w_h^2 \frac{(1 - w_h)}{n_h} s_{xh}^2$; $\hat{{y}_{st}} = \sum_{i = 1}^Hw_h\bar{y}_h$ and $\hat{{x}_{st}} = \sum_{i = 1}^Hw_h\bar{x}_h$.

```{r, echo=FALSE}
ch <- wh^2 * (1 - wh)/nh * vapply(xh, var, numeric(1))
bc <- sum(ch * bh)/sum(ch)
yCombinedRegressionStrata <- vapply(yh, mean, numeric(1)) + 
  bc * (mean(unlist(xuh)) - vapply(xh, mean, numeric(1)))
yCombinedRegression <- sum(wh * yCombinedRegressionStrata)
```

  The variance is 
  \[Var = \sum^H_{h = 1}[(\frac{N_h}{N})^2\frac{1 - f_h}{n_h(n_h-2)}\sigma_h^2]\]

  where $\sigma_h^2$ is 
  \[\sigma_h^2 = \sum_{i = 1}^{n_h}[(y_{hi} - {y}_h) - b_c(x_{hi} - {x}_h)^2]\]

```{r, echo=FALSE}
combinedRes <- lapply(H,
                      function(h) {
                        (yh[[h]] - mean(yh[[h]])) - 
                          bc * (xh[[h]] - mean(xh[[h]]))
                      })
varsCombinedRegressionStrata <- 
  getVarsRegression(wh, nh, combinedRes)
varsCombinedRegression <- sum(wh^2 * 
                                varsCombinedRegressionStrata)
```

# Results

```{r, echo=FALSE}
df <- data.frame(
  `Mean` = c(ySeparateRatio, yCombinedRatio,
             ySeparateRegression, yCombinedRegression),
  `Standard Variance` = c(sqrt(varSeparateRatio), 
                          sqrt(varCombinedRatio),
                          sqrt(varsSeparateRegression), 
                          sqrt(varsCombinedRegression))
)
rownames(df) <- c("Separate Ratio", 
                  "Combined Ratio",
                  "Separate Regression", 
                  "Combined Regression")
knitr::kable(df, digits = 4,
             caption = "Statistical Summary (log)")


payPerMile_SeparateRatio <- exp(ySeparateRatio)
payPerMile_CombinedRatio <- exp(yCombinedRatio)
payPerMile_SeparateRegression <- exp(ySeparateRegression)
payPerMile_CombinedRegression <- exp(yCombinedRegression)
varApprox <- function(var, mean) {
  var * exp(mean)^2 - exp(mean)^2/4 * var^2
}

sd_SeparateRatio <- sqrt(varApprox(varSeparateRatio, 
                                   payPerMile_SeparateRatio))
sd_CombinedRatio <- sqrt(varApprox(varCombinedRatio, 
                                   payPerMile_CombinedRatio))
sd_SeparateRegression <- sqrt(varApprox(varsSeparateRegression, 
                                        payPerMile_SeparateRegression))
sd_CombinedRegression <- sqrt(varApprox(varsCombinedRegression, 
                                        payPerMile_CombinedRegression))
df <- data.frame(
  `Mean` = c(payPerMile_SeparateRatio, payPerMile_CombinedRatio,
             payPerMile_SeparateRegression, payPerMile_CombinedRegression),
  `Standard Variance` = c(sd_SeparateRatio, 
                          sd_CombinedRatio,
                          sd_SeparateRegression, 
                          sd_CombinedRegression)
)
rownames(df) <- c("Separate Ratio", 
                  "Combined Ratio",
                  "Separate Regression", 
                  "Combined Regression")
knitr::kable(df, digits = 2,
             caption = "Statistical Summary")
```

Table 1 illustrates the statistical summary (log) of those four models and Table 2 shows the transformed statistical summary, we can tell that 

* the mean of all four estimates is around 7.03 which means that in general, the unit price of uber is around \$**7.03** ($\exp(1.95)$) per mile in Manhattan, in Jan, 2015.

* the standard variance of regression estimators is much smaller than that of the ratio estimators. Thus, compared with the ratio estimation, regression estimation is more robust.

* the difference of the `separate` and `combined` strategy is negligible.

Since the difference of strategy `separate` and `combined` is not obvious, we would only talk about one of each (i.e. Combined Ratio Estimation and Combined Regression Estimation)

```{r, echo=FALSE, fig.cap="Regession and Ratio Estimation Comparison", warning=FALSE, error = FALSE, message=FALSE, fig.width=8, fig.height=4, fig.align="center"}
groupMean <- function(x) {
  len <- length(x)
  if(!(len %% 2)) {
    len <- len/2
    vapply(seq(len),
           function(i) {
             (x[i] + x[i + len])/2
           }, numeric(1))
  } else stop("Even input")
}  

tb <- tibble(time = rep(factor(H-1), 2), 
             y = c(exp(ySeparateRatioStrata), 
                   exp(yCombinedRegressionStrata)),
             lower = c(exp(ySeparateRatioStrata) - 
                         1.96 * 
                         sqrt(
                           varApprox(varCombinedRatio, 
                                     exp(ySeparateRatioStrata))), 
                       exp(ySeparateRatioStrata) - 
                         1.96 * 
                         sqrt(
                           varApprox(varsCombinedRegressionStrata, 
                                     exp(yCombinedRegressionStrata)))),
             upper = c(exp(ySeparateRatioStrata) + 
                         1.96 * 
                         sqrt(varApprox(
                           varCombinedRatio, 
                           exp(ySeparateRatioStrata))), 
                       exp(ySeparateRatioStrata) + 
                         1.96 * 
                         sqrt(varApprox(
                           varsCombinedRegressionStrata, 
                           exp(yCombinedRegressionStrata)))),
             weight = rep(Nh, 2),
             group = rep(c("Combined Ratio", 
                           "Combined Regression"), 
                         each = 24)) 
tb %>% 
  ggplot(aes(x = time, y = y)) + 
  geom_errorbar(aes(ymin = lower, ymax = upper, colour = weight)) + 
  geom_hline(yintercept = payPerMile_SeparateRegression, 
             color = "grey50", alpha = 0.8) + 
  geom_hline(yintercept = payPerMile_SeparateRegression - 
               1.96 * sd_SeparateRegression, 
             color = "grey70", alpha = 0.5, linetype = 2) + 
  geom_hline(yintercept = payPerMile_SeparateRegression + 
               1.96 * sd_SeparateRegression, 
             color = "grey70", alpha = 0.5, linetype = 2) +
  facet_wrap(~group) + 
  ggtitle("Regression Estimator") + 
  scale_color_continuous(type = "viridis") + 
  theme(axis.text.x = element_text(angle = -45, vjust = 0.5, hjust=1))+
  ylab("Payment per Mile")
```

In Figure 4, the color of the bars represents the number of observations in each strata ($N_h$). The dark colour (i.e. purple) represents less $N_h$ and the bright colour (i.e. yellow) represents high $N_h$. The length of the bar represents the confidence interval $\mu \pm 1.96 \text{sd}$. The horizontal solid line represents the estimate mean of the overall 'price per mile' (both methods give very similar intervals). We could tell that

1. Combined Ratio estimator gives way larger variance than the Combined Regression Model. 

2. The variance at daily time (from 8 to 19) is larger than that at night (from 19 to 8).

2. The number of observations from 6 to 8 is the most and passages in the morning (5) are less activated.

3. In the day time, the payment per mile at 5 or 6 has lowest variance. Conversely, from 8 to 11, the variances are the largest which makes sense since in the morning rush hour, uber users are more likely to increase the rate to be on time to work.

# Discussion

## Conclusion

In conclusion, based on the New York uber data, the mean price is around \$7.03 per mile. We construct four models, separate/combined ratio estimation and separate/combined regression estimation based on the stratified sampling. The difference between separate and combined model is negligible, however, the difference between ratio and regression estimation is great, especially in standard deviation. The standard deviation of ratio estimation is around 12, however, in regression estimation, the standard deviation is only 0.6. We may conclude that compared with the ratio estimation, the regression estimation is more stable.

In addition, we found that, in general, people at 18 to 20 are more likely to call uber than other time. From 8 to 11, the price pre mile varies a lot which shows that people tend to pay higher price (give more tips) at this time. In contrast, from 0am to 6am, the price varies little (the number of passages is less as well) showing that the uber drives are more likely to get less tips than the morning time.

## Weekness

Our model has some weakness as well

* **Data**: In our case, the **study error** is mainly combined by two parts, *technique error* and *seasonal impact*. 

  + Through the data, we can find some tips are extremely large, even close to 4 million which is obviously impossible. It may be caused by uber system incorrect recording or some other technique issues. We need to set a boundary of the tip to remove such bugs. The choices are not arbitrary and reasonable at some manner. Since *Manhattan Island is 22.7 square miles in area, 13.4 miles (21.6 km) long and 2.3 miles (3.7 km) wide, at its widest (near 14th Street).*, in general, a possible trip distance in Manhattan Island should be no more than 30 miles. However, there could be some unusual trips larger than 30 miles (i.e. travel between the Manhattan Island and its surroundings). All these recordings are omitted manually.
  
  + The *seasonal impact* would be that since the collected data is only in Jan (presumably winter), the tips may also vary via seasons (i.e. in summer, people are less likely to call uber, etc) which we cannot obtain. 

* **Model**: we only picked two main models (ratio and regression). However, these two models have their own drawbacks.

  + Ratio estimation: ratio estimates are biased and corrections must be made. In our case, we do not adjust the biased issue.
  
  + Regression estimation: it has assumptions such that the residuals are normally distributed, the residuals should be independent, etc. In this project, we do not really check the model adequacy which should be done in the next move.
  
## The Next Steps

After fitting the regression model, we should do some residual checks, such as Augmented Dickey-Fuller test [@dickey1979distribution] for stationary, Box-Pierce test [@box1970distribution] for the independence, Jarque-Bera test [@jarque1980efficient] for the normality, etc.

Additionally, we should try to use some other models, such as `GLMM` (generalized linear mixed model) [@mcculloch2014generalized] to fit grouped data (set `hour` as the random effect). 


\newpage

# References
